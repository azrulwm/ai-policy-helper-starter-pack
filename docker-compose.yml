services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - '6333:6333'
    healthcheck:
      test: ['CMD-SHELL', "bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 20
    volumes:
      - qdrant_data:/qdrant/storage

  backend:
    build: ./backend
    environment:
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-local-384}
      - LLM_PROVIDER=${LLM_PROVIDER:-stub}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      - VECTOR_STORE=${VECTOR_STORE:-qdrant}
      - COLLECTION_NAME=${COLLECTION_NAME:-policy_helper}
      - CHUNK_SIZE=${CHUNK_SIZE:-700}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-80}
    volumes:
      - ./data:/app/data:ro
    ports:
      - '8000:8000'
    depends_on:
      qdrant:
        condition: service_healthy

  frontend:
    build: ./frontend
    environment:
      - NEXT_PUBLIC_API_BASE=${NEXT_PUBLIC_API_BASE:-http://localhost:8000}
    ports:
      - '3000:3000'
    depends_on:
      - backend

  ollama:
    image: ollama/ollama:latest
    ports:
      - '11434:11434'
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama-init.sh:/usr/local/bin/ollama-init.sh:ro
    entrypoint: ["/bin/bash", "/usr/local/bin/ollama-init.sh"]
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q llama3.2:1b || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 180s  # Give more time for model download
    profiles:
      - ollama

volumes:
  qdrant_data:
  ollama_data:
